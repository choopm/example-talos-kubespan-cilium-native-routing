version: 3

env:
  TALOSCONFIG: './talosconfig'
  KUBECONFIG: './kubeconfig'

vars:
  CONTROLPLANES:
    map:
      '1': '192.168.122.27'
  WORKERS:
    map:
      '1': '192.168.122.231'
      '2': '192.168.122.243'
      '3': '192.168.122.130'
  FACTORYIMAGE: factory.talos.dev/metal-installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.11.5
  CILIUMVERSION: 1.18.4
  NATIVEROUTINGCIDR: 10.244.0.0/16

tasks:
  bootstrap:
    desc: Bootstraps the cluster
    cmds:
      - task: apply
        vars:
          EXTRAARG: "--insecure"
      - talosctl config endpoint {{ index .CONTROLPLANES "1" }}
      - talosctl config node {{ index .CONTROLPLANES "1" }}
      - sleep 5
      - talosctl -n {{ index .CONTROLPLANES "1" }} bootstrap
      - sleep 5
      - talosctl -n {{ index .CONTROLPLANES "1" }} kubeconfig -f .

  apply:
    desc: (re)-applies machineconfigs to all nodes
    vars:
      EXTRAARG: "{{ .EXTRAARG }}"
    cmds:
      - for: { var: CONTROLPLANES }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: >
            apply-config {{.EXTRAARG}}
            --file controlplane.yaml
            --config-patch '[{"op": "add", "path": "/machine/network/hostname", "value": "talos-cp-{{ .KEY }}"}]'
      - for: { var: WORKERS }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: >
            apply-config {{.EXTRAARG}}
            --file worker.yaml
            --config-patch '[{"op": "add", "path": "/machine/network/hostname", "value": "talos-w-{{ .KEY }}"}]'

  reset:
    desc: Resets all nodes to maintenance
    cmds:
      - for: { var: WORKERS }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: >
            reset --reboot=true --graceful=false
            --system-labels-to-wipe STATE
            --system-labels-to-wipe EPHEMERAL
            || true
      - for: { var: CONTROLPLANES }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: >
            reset --reboot=true --graceful=false
            --system-labels-to-wipe STATE
            --system-labels-to-wipe EPHEMERAL
            || true

  patch:
    desc: Applies a patch against all configured nodes
    vars:
      PATCH: "{{ .PATCH }}"
    requires:
      vars:
        - PATCH
    cmds:
      - for: { var: CONTROLPLANES }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: patch machineconfig --patch-file {{ .PATCH }}
      - for: { var: WORKERS }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: patch machineconfig --patch-file {{ .PATCH }}

  gen-talos:
    desc: Generates talos configurations
    deps:
      - gen-talos-secrets
      - gen-cilium
    cmds:
      - |
        talosctl gen config \
          test https://{{ index .CONTROLPLANES "1" }}:6443 \
          --force \
          --with-examples=true \
          --with-docs=true \
          --with-cluster-discovery=true \
          --with-kubespan=true \
          --with-secrets secrets.yaml \
          --install-image {{ .FACTORYIMAGE }} \
          --config-patch-control-plane @patches/cni.yaml \
          --config-patch-control-plane @patches/cilium.yaml \
          --config-patch-control-plane @patches/cilium-host-node-cidr.yaml \
          --config-patch @patches/kubespan.yaml \
          --config-patch @patches/disk.yaml \
          --config-patch @patches/dns.yaml \
          --config-patch @patches/time.yaml

  all:
    desc: "Runs talosctl with arguments against all nodes, example: `task all -- get disks`"
    cmds:
      - for: { var: CONTROLPLANES }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: "{{ .CLI_ARGS }}"
      - for: { var: WORKERS }
        task: talosctl
        vars:
          NODE: "{{ .ITEM }}"
          ARGS: "{{ .CLI_ARGS }}"

  gen-talos-secrets:
    desc: Generates talos secrets.yaml if missing
    internal: true
    status:
      - test -f secrets.yaml
    cmd: talosctl gen secrets -o secrets.yaml

  talosctl:
    desc: Wraps talosctl to run against endpoint and node
    internal: true
    vars:
      NODE: "{{ .NODE }}"
      ARGS: "{{ .ARGS }}"
    requires:
      vars:
        - NODE
        - ARGS
    cmd: talosctl -e {{ .NODE }} -n {{ .NODE }} {{ .ARGS }}

  gen-cilium:
    desc: Renders cilium manifests to be used during upgrades
    cmds:
      - helm repo update cilium
      - |
        cat << 'EOF' > ./patches/cilium.yaml
        # # required when re-applying as a patch
        # cluster:
        #   inlineManifests:
        #     - name: cilium
        #       $patch: delete
        # ---
        EOF
      - |
        helm template cilium cilium/cilium \
          --version {{ .CILIUMVERSION }} \
          --namespace kube-system \
          --set=ipam.mode=kubernetes \
          --set=kubeProxyReplacement=true \
          --set=securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
          --set=securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
          --set=cgroup.autoMount.enabled=false \
          --set=cgroup.hostRoot=/sys/fs/cgroup \
          --set=k8sServiceHost=localhost \
          --set=k8sServicePort=7445 \
          --set=ipv6.enabled=false \
          --set=enableIPv6Masquerade=false \
          \
          --set=bgpControlPlane.enabled=true \
          --set=defaultLBServiceIPAM=none \
          --set=bpf.lbExternalClusterIP=true \
          --set=nodeIPAM.enabled=true \
          --set=l2announcements.enabled=true \
          --set=externalIPs.enabled=true \
          \
          --set=routingMode=native \
          --set=ipv4NativeRoutingCIDR={{ .NATIVEROUTINGCIDR }} \
          --set=bpf.masquerade=true \
          --set=bpf.hostLegacyRouting=true \
        | yq -n '.cluster.inlineManifests[0].name="cilium", .cluster.inlineManifests[0].contents=(load_str("/dev/stdin") | sub(" *\n", "\n"))' \
        >> ./patches/cilium.yaml

  deploy-example:
    desc: Deloy example workload
    cmds:
      - kubectl apply -k example-workload

  test-example:
    desc: Test example workload
    cmds:
      # try first address of node ipam lb
      - |
        NODE_IP=$(kubectl get svc example-workload -o jsonpath='{.status.loadBalancer.ingress[0].ip}'; echo)
        NODE_PORT=$(kubectl get svc example-workload -o jsonpath='{.spec.ports[0].port}')
        curl http://$NODE_IP:$NODE_PORT
      # try third address of node ipam lb, should be working too
      - |
        NODE_IP=$(kubectl get svc example-workload -o jsonpath='{.status.loadBalancer.ingress[2].ip}'; echo)
        NODE_PORT=$(kubectl get svc example-workload -o jsonpath='{.spec.ports[0].port}')
        curl http://$NODE_IP:$NODE_PORT
